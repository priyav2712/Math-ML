{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95061ed3",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Level 9 ‚Äî Adaptive Optimizers: RMSProp vs Adam vs AdamW\n",
    "\n",
    "> **Objective:**  \n",
    "> To visualize and compare the behavior of **RMSProp**, **Adam**, and **AdamW** optimizers  \n",
    "> on a curved 3D loss surface, highlighting how adaptive learning rates and weight decay  \n",
    "> influence convergence speed, path smoothness, and stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7c8be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Define 3D loss surface\n",
    "def f(x, y):\n",
    "    \"\"\"Non-convex loss surface.\"\"\"\n",
    "    return np.log(1 + x**2 + 2*y**2) + 0.3*np.sin(3*x) * np.cos(3*y)\n",
    "\n",
    "def grad(x, y):\n",
    "    \"\"\"Gradient of the loss function.\"\"\"\n",
    "    dfdx = (2*x / (1 + x**2 + 2*y**2)) + 0.9*np.cos(3*x)*np.cos(3*y)\n",
    "    dfdy = (4*y / (1 + x**2 + 2*y**2)) - 0.9*np.sin(3*x)*np.sin(3*y)\n",
    "    return np.array([dfdx, dfdy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713d50b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def rmsprop(start, lr=0.08, beta=0.9, eps=1e-8, steps=70):\n",
    "    x, y = start\n",
    "    s = np.zeros(2)\n",
    "    path = [(x, y)]\n",
    "    for _ in range(steps):\n",
    "        g = grad(x, y)\n",
    "        s = beta * s + (1 - beta) * (g ** 2)\n",
    "        x -= lr * g[0] / (np.sqrt(s[0]) + eps)\n",
    "        y -= lr * g[1] / (np.sqrt(s[1]) + eps)\n",
    "        path.append((x, y))\n",
    "    return np.array(path)\n",
    "\n",
    "def adam(start, lr=0.08, beta1=0.9, beta2=0.999, eps=1e-8, steps=70):\n",
    "    x, y = start\n",
    "    m = np.zeros(2)\n",
    "    v = np.zeros(2)\n",
    "    path = [(x, y)]\n",
    "    for t in range(1, steps + 1):\n",
    "        g = grad(x, y)\n",
    "        m = beta1 * m + (1 - beta1) * g\n",
    "        v = beta2 * v + (1 - beta2) * (g ** 2)\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        x -= lr * m_hat[0] / (np.sqrt(v_hat[0]) + eps)\n",
    "        y -= lr * m_hat[1] / (np.sqrt(v_hat[1]) + eps)\n",
    "        path.append((x, y))\n",
    "    return np.array(path)\n",
    "\n",
    "def adamw(start, lr=0.08, beta1=0.9, beta2=0.999, weight_decay=0.02, eps=1e-8, steps=70):\n",
    "    x, y = start\n",
    "    m = np.zeros(2)\n",
    "    v = np.zeros(2)\n",
    "    path = [(x, y)]\n",
    "    for t in range(1, steps + 1):\n",
    "        g = grad(x, y)\n",
    "        # Apply weight decay directly to parameters (decoupled)\n",
    "        g += weight_decay * np.array([x, y])\n",
    "        m = beta1 * m + (1 - beta1) * g\n",
    "        v = beta2 * v + (1 - beta2) * (g ** 2)\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        x -= lr * m_hat[0] / (np.sqrt(v_hat[0]) + eps)\n",
    "        y -= lr * m_hat[1] / (np.sqrt(v_hat[1]) + eps)\n",
    "        path.append((x, y))\n",
    "    return np.array(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42264e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start = np.array([2.8, 2.5])\n",
    "steps = 70\n",
    "\n",
    "rms_path = rmsprop(start, steps=steps)\n",
    "adam_path = adam(start, steps=steps)\n",
    "adamw_path = adamw(start, steps=steps)\n",
    "\n",
    "paths = {\"RMSProp\": rms_path, \"Adam\": adam_path, \"AdamW\": adamw_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f5970",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Surface grid\n",
    "X = np.linspace(-3, 3, 150)\n",
    "Y = np.linspace(-3, 3, 150)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "ax.plot_surface(X, Y, Z, cmap=\"plasma\", alpha=0.7, linewidth=0.4)\n",
    "ax.set_title(\"3D Adaptive Optimizers: RMSProp vs Adam vs AdamW\", fontsize=13)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"Loss\")\n",
    "\n",
    "colors = {\"RMSProp\": \"cyan\", \"Adam\": \"orange\", \"AdamW\": \"red\"}\n",
    "lines, points = {}, {}\n",
    "\n",
    "for name in paths:\n",
    "    lines[name], = ax.plot([], [], [], lw=2, color=colors[name], label=name)\n",
    "    points[name], = ax.plot([], [], [], \"o\", color=colors[name])\n",
    "\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dcba2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    for name in paths:\n",
    "        lines[name].set_data([], [])\n",
    "        lines[name].set_3d_properties([])\n",
    "        points[name].set_data([], [])\n",
    "        points[name].set_3d_properties([])\n",
    "    return list(lines.values()) + list(points.values())\n",
    "\n",
    "def update(frame):\n",
    "    for name, path in paths.items():\n",
    "        if frame >= len(path):\n",
    "            continue\n",
    "        x, y = path[:frame, 0], path[:frame, 1]\n",
    "        z = f(x, y)\n",
    "        lines[name].set_data(x, y)\n",
    "        lines[name].set_3d_properties(z)\n",
    "        points[name].set_data(x[-1:], y[-1:])\n",
    "        points[name].set_3d_properties(z[-1:])\n",
    "    return list(lines.values()) + list(points.values())\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, init_func=init,\n",
    "                              frames=steps, interval=150, blit=False)\n",
    "\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "plt.close(fig)\n",
    "matplotlib.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "display(HTML(ani.to_jshtml()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341d4f5",
   "metadata": {},
   "source": [
    "## üß† Mathematical Insight\n",
    "\n",
    "### RMSProp\n",
    "- Adapts learning rate using moving average of squared gradients:\n",
    "  $$\n",
    "  s_t = \\beta s_{t-1} + (1 - \\beta)(\\nabla f_t)^2\n",
    "  $$\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\frac{\\nabla f_t}{\\sqrt{s_t} + \\epsilon}\n",
    "  $$\n",
    "\n",
    "### Adam\n",
    "- Adds momentum on top of RMSProp:\n",
    "  $$\n",
    "  m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)\\nabla f_t, \\quad\n",
    "  v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)(\\nabla f_t)^2\n",
    "  $$\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\frac{m_t / (1 - \\beta_1^t)}{\\sqrt{v_t / (1 - \\beta_2^t)} + \\epsilon}\n",
    "  $$\n",
    "\n",
    "### AdamW\n",
    "- Decouples **weight decay** from gradient updates:\n",
    "  $$\n",
    "  \\nabla f_t \\leftarrow \\nabla f_t + \\lambda \\theta_t\n",
    "  $$\n",
    "  which prevents **L2 regularization bias** in Adam,  \n",
    "  yielding **better generalization** and **stable training**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1acae6",
   "metadata": {},
   "source": [
    "## üß© Key Observations\n",
    "\n",
    "| Optimizer | Descent Path | Adaptivity | Stability | Comment |\n",
    "|------------|--------------|-------------|-------------|----------|\n",
    "| **RMSProp** | Fluctuates mildly | ‚úÖ Adaptive | ‚ö†Ô∏è Slight noise | Good, but lacks momentum |\n",
    "| **Adam** | Smooth adaptive descent | ‚úÖ Adaptive | ‚úÖ Stable | Fast convergence |\n",
    "| **AdamW** | Smoothest, steady | ‚úÖ Adaptive + Regularized | ‚úÖ Most Stable | Best generalization & balance |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de277e0",
   "metadata": {},
   "source": [
    "## üß≠ Takeaway\n",
    "\n",
    "- **RMSProp** introduced adaptive learning rates.  \n",
    "- **Adam** fused momentum + adaptive updates for fast convergence.  \n",
    "- **AdamW** further improved it by decoupling weight decay ‚Äî reducing bias and improving generalization.\n",
    "\n",
    "**Modern deep learning models (Transformers, CNNs, LLMs)** use **AdamW** as their default optimizer.  \n",
    "This level connects pure mathematical optimization to large-scale model training in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f83b16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ani.save(\"adaptive_optimizers_3D.gif\", writer=\"pillow\", fps=8)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
