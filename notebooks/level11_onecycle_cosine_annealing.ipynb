{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbaf4a22",
   "metadata": {},
   "source": [
    "# üåå Level 11 ‚Äî OneCycleLR and Cosine Annealing (Warm Restarts)\n",
    "\n",
    "> **Objective:**  \n",
    "> To understand and visualize advanced learning rate control techniques:  \n",
    "> **OneCycleLR** and **Cosine Annealing with Warm Restarts**,  \n",
    "> which dynamically vary the learning rate (and momentum) during training  \n",
    "> to achieve faster convergence and better generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6801e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ccd671",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def one_cycle_lr(t, total_steps=100, max_lr=0.1, min_lr=0.01, pct_up=0.3):\n",
    "    \"\"\"Implements Leslie Smith‚Äôs OneCycle policy.\"\"\"\n",
    "    up_steps = int(total_steps * pct_up)\n",
    "    down_steps = total_steps - up_steps\n",
    "    \n",
    "    if t < up_steps:\n",
    "        return min_lr + (max_lr - min_lr) * (t / up_steps)\n",
    "    else:\n",
    "        return max_lr - (max_lr - min_lr) * ((t - up_steps) / down_steps)\n",
    "\n",
    "\n",
    "def cosine_annealing_lr(t, T_max=50, max_lr=0.1, min_lr=0.01):\n",
    "    \"\"\"Cosine annealing learning rate without restarts.\"\"\"\n",
    "    return min_lr + (max_lr - min_lr) * (1 + np.cos(np.pi * t / T_max)) / 2\n",
    "\n",
    "\n",
    "def cosine_annealing_warm_restarts(t, T_0=30, T_mult=2, max_lr=0.1, min_lr=0.01):\n",
    "    \"\"\"Cosine annealing with warm restarts (SGDR).\"\"\"\n",
    "    Ti = T_0\n",
    "    cycle = 0\n",
    "    while t >= Ti:\n",
    "        t -= Ti\n",
    "        Ti *= T_mult\n",
    "        cycle += 1\n",
    "    return min_lr + (max_lr - min_lr) * (1 + np.cos(np.pi * t / Ti)) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47ab01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "steps = np.arange(0, 150)\n",
    "\n",
    "lr_onecycle = [one_cycle_lr(t, total_steps=150, max_lr=0.08, min_lr=0.005, pct_up=0.3) for t in steps]\n",
    "lr_cosine = [cosine_annealing_lr(t, T_max=100, max_lr=0.08, min_lr=0.005) for t in steps]\n",
    "lr_warm = [cosine_annealing_warm_restarts(t, T_0=30, T_mult=2, max_lr=0.08, min_lr=0.005) for t in steps]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(steps, lr_onecycle, label=\"OneCycleLR\", lw=2)\n",
    "plt.plot(steps, lr_cosine, label=\"Cosine Annealing\", lw=2)\n",
    "plt.plot(steps, lr_warm, label=\"Cosine Warm Restarts\", lw=2)\n",
    "\n",
    "plt.title(\"Advanced Learning Rate Schedules\", fontsize=13)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Learning Rate (Œ∑)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60e27b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def one_cycle_momentum(t, total_steps=100, max_m=0.95, min_m=0.85, pct_up=0.3):\n",
    "    \"\"\"Momentum schedule used in OneCycleLR ‚Äî inverse to LR.\"\"\"\n",
    "    up_steps = int(total_steps * pct_up)\n",
    "    down_steps = total_steps - up_steps\n",
    "    if t < up_steps:\n",
    "        return max_m - (max_m - min_m) * (t / up_steps)\n",
    "    else:\n",
    "        return min_m + (max_m - min_m) * ((t - up_steps) / down_steps)\n",
    "\n",
    "momentum_schedule = [one_cycle_momentum(t, total_steps=150) for t in steps]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(steps, lr_onecycle, label=\"Learning Rate\", color=\"blue\")\n",
    "plt.plot(steps, momentum_schedule, label=\"Momentum\", color=\"red\")\n",
    "plt.title(\"OneCycle Policy: Coupled LR‚ÄìMomentum Schedule\", fontsize=13)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514b393",
   "metadata": {},
   "source": [
    "## üß† Mathematical Insight\n",
    "\n",
    "### üîπ OneCycleLR (Leslie Smith, 2018)\n",
    "Learning rate increases, then decreases symmetrically:\n",
    "$$\n",
    "\\eta_t =\n",
    "\\begin{cases}\n",
    "\\eta_{min} + (\\eta_{max} - \\eta_{min}) \\cdot \\frac{t}{T_{up}} & \\text{if } t < T_{up} \\\\\n",
    "\\eta_{max} - (\\eta_{max} - \\eta_{min}) \\cdot \\frac{t - T_{up}}{T_{down}} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Momentum is varied *inversely*:\n",
    "$$\n",
    "m_t \\propto 1 - \\eta_t\n",
    "$$\n",
    "\n",
    "‚Üí High LR + low momentum = exploration  \n",
    "‚Üí Low LR + high momentum = fine-tuning  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Cosine Annealing\n",
    "Learning rate follows a cosine curve:\n",
    "$$\n",
    "\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 + \\cos(\\pi t / T))\n",
    "$$\n",
    "\n",
    "This smooth ‚Äúwave‚Äù prevents sudden jumps and keeps the optimizer dynamically adaptive.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Warm Restarts (SGDR)\n",
    "When training is long, cosine cycles restart periodically:\n",
    "$$\n",
    "\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 + \\cos(\\pi t / T_i))\n",
    "$$\n",
    "where \\( T_i = T_0 \\times T_{mult}^{cycle} \\).\n",
    "\n",
    "Each restart injects *energy* into the system,  \n",
    "helping the optimizer escape sharp local minima while maintaining convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3045a1c6",
   "metadata": {},
   "source": [
    "## üß© Comparison of Advanced LR Strategies\n",
    "\n",
    "| Schedule | Curve Shape | Key Benefit | Typical Use |\n",
    "|-----------|--------------|--------------|--------------|\n",
    "| **OneCycleLR** | Triangular up‚Äìdown | Fastest convergence, avoids saddle points | CNNs, Transformers |\n",
    "| **Cosine Annealing** | Smooth decay | Stable training, avoids oscillations | Medium-long epochs |\n",
    "| **Cosine Annealing (Warm Restarts)** | Recurrent waves | Rejuvenates training, improves generalization | Long training regimes |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d4195",
   "metadata": {},
   "source": [
    "## üß≠ Takeaway\n",
    "\n",
    "- **OneCycleLR** dynamically balances exploration and refinement within one training cycle.  \n",
    "- **Cosine Annealing** ensures smooth, low-noise convergence.  \n",
    "- **Warm Restarts** reintroduce exploration bursts in long runs.  \n",
    "\n",
    "Together, they form the *modern foundation of learning rate control*  \n",
    "‚Äî used in state-of-the-art architectures like **ResNet, BERT, GPT, and ViT**.\n",
    "\n",
    "> ‚ÄúAn optimizer is not just about gradient steps ‚Äî it‚Äôs about energy choreography.‚Äù\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
