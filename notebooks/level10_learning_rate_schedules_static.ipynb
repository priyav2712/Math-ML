{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56c2402",
   "metadata": {},
   "source": [
    "# âš¡ Level 10 â€” Learning Rate Schedules (Static Visualization)\n",
    "\n",
    "> **Objective:**  \n",
    "> Understand how different learning-rate scheduling strategies â€”  \n",
    "> **Constant**, **Step Decay**, **Exponential Decay**, and **Cyclical (Triangular)** â€”  \n",
    "> influence gradient descent convergence on a convex surface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da9bba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple convex bowl-shaped loss surface\n",
    "def f(x, y): \n",
    "    return 0.5 * (x**2 + 2*y**2)\n",
    "\n",
    "def grad(x, y):\n",
    "    return np.array([x, 2*y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f213c22",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def constant_lr(t, base_lr=0.1):\n",
    "    return base_lr\n",
    "\n",
    "def step_decay_lr(t, base_lr=0.1, drop=0.5, step_size=20):\n",
    "    return base_lr * (drop ** (t // step_size))\n",
    "\n",
    "def exp_decay_lr(t, base_lr=0.1, decay=0.05):\n",
    "    return base_lr * np.exp(-decay * t)\n",
    "\n",
    "def cyclical_lr(t, base_lr=0.01, max_lr=0.1, step_size=15):\n",
    "    cycle = np.floor(1 + t / (2 * step_size))\n",
    "    x = np.abs(t / step_size - 2 * cycle + 1)\n",
    "    return base_lr + (max_lr - base_lr) * max(0, (1 - x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b69d8e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_gd(lr_schedule, steps=80, base_lr=0.1):\n",
    "    x, y = 2.5, 2.5\n",
    "    path = [(x, y, f(x, y))]\n",
    "    for t in range(steps):\n",
    "        lr = lr_schedule(t)\n",
    "        g = grad(x, y)\n",
    "        x -= lr * g[0]\n",
    "        y -= lr * g[1]\n",
    "        path.append((x, y, f(x, y)))\n",
    "    return np.array(path)\n",
    "\n",
    "paths = {\n",
    "    \"Constant\": run_gd(lambda t: constant_lr(t, 0.08)),\n",
    "    \"Step Decay\": run_gd(lambda t: step_decay_lr(t, 0.08, drop=0.5, step_size=20)),\n",
    "    \"Exponential\": run_gd(lambda t: exp_decay_lr(t, 0.08, decay=0.05)),\n",
    "    \"Cyclical\": run_gd(lambda t: cyclical_lr(t, 0.01, 0.08, 15)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14177ee9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "t = np.arange(80)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(t, [constant_lr(i, 0.08) for i in t], label=\"Constant\", lw=2)\n",
    "plt.plot(t, [step_decay_lr(i, 0.08) for i in t], label=\"Step Decay\", lw=2)\n",
    "plt.plot(t, [exp_decay_lr(i, 0.08) for i in t], label=\"Exponential\", lw=2)\n",
    "plt.plot(t, [cyclical_lr(i, 0.01, 0.08, 15) for i in t], label=\"Cyclical\", lw=2)\n",
    "\n",
    "plt.title(\"Learning Rate vs Iterations\", fontsize=13)\n",
    "plt.xlabel(\"Iteration (t)\")\n",
    "plt.ylabel(\"Learning Rate (Î·)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc39e79",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for name, path in paths.items():\n",
    "    plt.plot(path[:, 2], label=name, lw=2)\n",
    "\n",
    "plt.title(\"Loss Convergence under Different Learning Rate Schedules\", fontsize=13)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss f(x, y)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7718dd5f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "for name, path in paths.items():\n",
    "    plt.plot(path[:, 0], path[:, 1], label=name, lw=2)\n",
    "    plt.scatter(path[-1, 0], path[-1, 1], marker=\"o\", color=\"black\", s=30)\n",
    "plt.title(\"Parameter Trajectories under Different LR Schedules\", fontsize=13)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0829b",
   "metadata": {},
   "source": [
    "## ðŸ§  Mathematical Insight\n",
    "\n",
    "| Schedule | Update Rule | Key Characteristic |\n",
    "|-----------|--------------|--------------------|\n",
    "| **Constant** | \\( \\eta_t = \\eta_0 \\) | Fixed step size, simple but risky |\n",
    "| **Step Decay** | \\( \\eta_t = \\eta_0 \\times \\text{drop}^{\\lfloor t / s \\rfloor} \\) | Sharp reductions at fixed steps |\n",
    "| **Exponential Decay** | \\( \\eta_t = \\eta_0 e^{-\\lambda t} \\) | Smooth continuous decay |\n",
    "| **Cyclical** | \\( \\eta_t = \\eta_{min} + (\\eta_{max}-\\eta_{min}) \\max(0,1-|x|) \\) | Oscillates between exploration and fine-tuning |\n",
    "\n",
    "The **learning rate (Î·)** acts like *temperature in simulated annealing*:  \n",
    "- High â†’ exploration (escape local minima)  \n",
    "- Low â†’ exploitation (precise convergence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c649e1",
   "metadata": {},
   "source": [
    "## ðŸ§© Observations\n",
    "\n",
    "| Schedule | Strength | Weakness | Ideal Scenario |\n",
    "|-----------|-----------|-----------|----------------|\n",
    "| **Constant** | Fast early convergence | Unstable, may diverge | Small problems, low noise |\n",
    "| **Step Decay** | Sudden precision boost | Abrupt shifts | Classic ML setups |\n",
    "| **Exponential** | Smooth control | Can decay too early | Long steady training |\n",
    "| **Cyclical** | Escapes local minima | Slight oscillation | Deep learning (CNNs, NLP) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccdd111",
   "metadata": {},
   "source": [
    "## ðŸ§­ Takeaway\n",
    "\n",
    "- The **learning rate schedule** is the *control system* of optimization.\n",
    "- It determines how energy (gradient step) is allocated through time.\n",
    "- **Static LR** = simple but brittle.  \n",
    "- **Dynamic LR** = adaptive, robust, generalizable.\n",
    "\n",
    "> The best modern strategies combine *AdamW + Cyclical / Cosine Decay*  \n",
    "> â€” this is what powers stable, efficient training in real-world models like **ResNet**, **ViT**, and **Transformers**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
