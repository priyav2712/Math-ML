{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dba38c2",
   "metadata": {},
   "source": [
    "# Visualizing the Loss Surface in 3D for Linear Regression\n",
    "\n",
    "### Objective\n",
    "To understand how **Gradient Descent** moves across the **Mean Squared Error (MSE) loss surface**  \n",
    "for linear regression by plotting:\n",
    "1. The 3D loss landscape \\( J(w,b) \\)\n",
    "2. The descent trajectory of parameters \\((w,b)\\)\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "For data \\( (x_i, y_i) \\), the MSE loss is:\n",
    "\n",
    "\\[\n",
    "J(w, b) = \\frac{1}{N} \\sum_{i=1}^N (y_i - (wx_i + b))^2\n",
    "\\]\n",
    "\n",
    "Gradient Descent updates:\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "w &:= w - \\eta \\frac{\\partial J}{\\partial w} \\\\\n",
    "b &:= b - \\eta \\frac{\\partial J}{\\partial b}\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "The loss surface \\( J(w, b) \\) is a **convex paraboloid** ‚Äî  \n",
    "the algorithm moves downhill along this curved surface toward the global minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd637da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# --- Step 1: Generate synthetic data ---\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 50)\n",
    "true_w, true_b = 2.5, 5\n",
    "y = true_w * X + true_b + np.random.randn(*X.shape) * 2.0\n",
    "\n",
    "# --- Step 2: Define loss function ---\n",
    "def compute_loss(w, b):\n",
    "    y_pred = w * X + b\n",
    "    return np.mean((y - y_pred)**2)\n",
    "\n",
    "# --- Step 3: Create grid of w and b values for visualization ---\n",
    "W = np.linspace(0, 5, 100)\n",
    "B = np.linspace(0, 10, 100)\n",
    "WW, BB = np.meshgrid(W, B)\n",
    "ZZ = np.array([[compute_loss(w, b) for w in W] for b in B])\n",
    "\n",
    "# --- Step 4: Gradient Descent optimization ---\n",
    "w, b = 0.0, 0.0\n",
    "lr = 0.05\n",
    "epochs = 30\n",
    "path = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_pred = w * X + b\n",
    "    dw = -2 * np.mean(X * (y - y_pred))\n",
    "    db = -2 * np.mean(y - y_pred)\n",
    "    w -= lr * dw\n",
    "    b -= lr * db\n",
    "    path.append((w, b, compute_loss(w, b)))\n",
    "\n",
    "path = np.array(path)\n",
    "\n",
    "# --- Step 5: 3D Surface Visualization ---\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.plot_surface(WW, BB, ZZ, cmap='viridis', alpha=0.8)\n",
    "ax.plot(path[:,0], path[:,1], path[:,2], color='r', marker='o', markersize=4, linewidth=2, label='GD Path')\n",
    "ax.set_title('3D Loss Surface and Gradient Descent Path')\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_zlabel('Loss')\n",
    "ax.legend()\n",
    "\n",
    "# --- Step 6: 2D Contour Plot ---\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "contours = ax2.contour(WW, BB, ZZ, levels=40, cmap='viridis')\n",
    "ax2.plot(path[:,0], path[:,1], color='r', marker='o', linewidth=2, label='Descent Path')\n",
    "ax2.set_title('2D Contour View of Loss Surface')\n",
    "ax2.set_xlabel('w')\n",
    "ax2.set_ylabel('b')\n",
    "ax2.legend()\n",
    "plt.colorbar(contours, ax=ax2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final results\n",
    "print(f\"Final parameters: w ‚âà {path[-1,0]:.3f}, b ‚âà {path[-1,1]:.3f}, Loss ‚âà {path[-1,2]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2933c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîç Interpretation\n",
    "\n",
    "- The 3D paraboloid represents the **loss surface** for linear regression.\n",
    "- Each red marker is a gradient descent step ‚Äî showing how the optimizer slides down the surface.\n",
    "- In the contour plot, you can see the same movement projected onto the \\( (w, b) \\)-plane.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Conceptual Bridge\n",
    "\n",
    "| Optimization Concept | Machine Learning Equivalent |\n",
    "|----------------------|-----------------------------|\n",
    "| Newton‚Äôs Method | Second-order optimization (used in quasi-Newton solvers) |\n",
    "| Gradient Descent | Backpropagation in neural networks |\n",
    "| Loss Surface | Error landscape of model parameters |\n",
    "| Convergence | Training stability and generalization |\n",
    "\n",
    "This notebook unites **mathematics, visualization, and ML training intuition** ‚Äî  \n",
    "the same principles that underlie **IISc CDS** and **IIT Bombay CMInDS** interviews.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step (Level 6)**: Implement **Polynomial Regression (degree 2)** and visualize **overfitting vs. underfitting** with train/test data.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
